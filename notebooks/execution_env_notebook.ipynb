{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run base.ipynb\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from abides_gym_custom_execution_environment import SubGymMarketsCustomExecutionEnv\n",
    "from sigqlearning_qfunctions import SigQFunction\n",
    "from sigqlearning_test_execution import test\n",
    "from sigqlearning_train_execution import train\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the environment as gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register execution env for gym use \n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id=\"custom-execution-v0\",\n",
    "    entry_point=SubGymMarketsCustomExecutionEnv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to generate environment with certain parameters\n",
    "def generate_env(seed=None):\n",
    "    \"\"\"\n",
    "    generates specific environment with the parameters defined and set the seed\n",
    "    \"\"\"\n",
    "    env = gym.make(\n",
    "            \"custom-execution-v0\",\n",
    "            background_config=\"rmsc04\",\n",
    "            mkt_close=\"10:06:00\",\n",
    "            timestep_duration=\"10s\",\n",
    "            first_interval=\"00:05:00\",\n",
    "            observation_interval=\"00:01:00\",            \n",
    "            order_fixed_size=100,\n",
    "            max_inventory=2000,\n",
    "            starting_inventory=1400,\n",
    "            terminal_inventory_reward=-0.5, # reward or penalty\n",
    "            terminal_inventory_mode='quadratic', # linear, flat\n",
    "            running_inventory_reward_dampener=0., # 0.6, 1.0\n",
    "            damp_mode=\"symmetric\", # asymmetric\n",
    "            debug_mode=False\n",
    "        )\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the custom execution environment\n",
    "env = generate_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Learn a Q-function estimate\n",
    "Learn a Q-function estimate with Signature-Q-learning. We run 5 different seeds.\n",
    "\n",
    "**Note**: This code cell takes a long time to run. Would recommend to copy this cell in a python script and let it run in the back with [screen](https://linuxize.com/post/how-to-use-linux-screen/) or [tmux](https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/) for instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature parameters\n",
    "sigq_params = dict(\n",
    "    sig_depth = 7,\n",
    "    basepoint = True, \n",
    "    initial_bias = 0.1\n",
    ")\n",
    "\n",
    "# training parameter\n",
    "training_params = dict(\n",
    "    episodes = 2000,\n",
    "    discount=1.0,\n",
    "    learning_rate = 1e-4,\n",
    "    learning_rate_decay = dict(mode='linear', end_value=1e-7, epochs=1900), \n",
    "        #dict(mode='exponential', factor=0.998, end_value=1e-07),\n",
    "    exploration = 'greedy',\n",
    "    epsilon = 0.8,\n",
    "    epsilon_decay = dict(mode='linear', end_value=0.02, epochs=1000),\n",
    "    decay_mode = 'episodes',\n",
    "    debug_mode = None,\n",
    "    progress_display = 'livelossplot'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training runs\n",
    "n_runs = 5\n",
    "\n",
    "# dict to store results\n",
    "training_results = {run : [] for run in range(n_runs)}\n",
    "training_seeds = {\n",
    "    run : seed for run,seed \n",
    "    in zip(training_results, utils.generate_prime_seeds(n_runs))\n",
    "}\n",
    "final_Q_functions = {}\n",
    "\n",
    "# training runs\n",
    "runs_pbar = tqdm(training_results.keys())\n",
    "for run in runs_pbar:\n",
    "    runs_pbar.set_description(f\"Training run\")\n",
    "    \n",
    "    env = generate_env(training_seeds[run])\n",
    "    sigpolicy = SigQFunction(env, **sigq_params)\n",
    "    training_results[run] = train(env, sigpolicy, **training_params)\n",
    "    final_Q_functions[run] = sigpolicy.state_dict()\n",
    "    del sigpolicy, env # for safety in case env.reset() does not work properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save training results\n",
    "\n",
    "**Note**: Training results are saved under `file_name` concatenated with current date and time up to seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"custom_execution_v0_training_no_dampening\"\n",
    "utils.save_results(training_results, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_execution_v0_full_dampening\n",
    "# 5 runs, i_0 = 400, oder_size=100, execution action space, with convergence to optimal policy, trunc at 7, expl 500 epis, eps_start 0.8, eps_end 0.02, lr 1e-4 to 1e-6 in 1000 epis , epis 1000, seeds 3,2,5,7,1, quadratic penalty C=-1, basepoint=T, discount=1., damp=1.\n",
    "\n",
    "# custom_execution_v0_training_no_dampening\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with convergence to suboptimal policy, trunc at 7, expl 500 epis, eps_start 0.8, eps_end 0.02, lr 1e-4 to 1e-7 in 1000 epis , epis 1000, seeds (1,2) , quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "# both runs not fully optimal, first only reduced down to 200, second overshoots then buys again and ends around -100\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_2\n",
    "# 1 run, i_0 = 400, oder_size=100, execution action space, with convergence to optimal policy, trunc at 7, expl 500 epis, eps_start 0.8, eps_end 0.02, lr 1e-4 to 1e-6 in 1000 epis , epis 1000, seeds 1 , quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_3\n",
    "# 1 run, i_0 = 400, oder_size=100, execution action space, with convergence to SUBoptimal policy, trunc at 7, expl 500 epis, eps_start 0.8, eps_end 0.02, lr 1e-4 to 1e-6 in 1000 epis , epis 1000, seeds 1 , quadratic penalty C=-1, basepoint=T, discount=.99, damp=0.\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_4\n",
    "# 5 runs, i_0 = 400, oder_size=100, execution action space, some runs with convergence to suboptimal policy, trunc at 7, expl 500 epis, eps_start 0.8, eps_end 0.02, lr 1e-4 to 1e-6 in 1000 epis , epis 1000, seeds 1,2,3,5,7 , quadratic penalty C=-1, basepoint=T, discount=.99, damp=0.\n",
    "# unstable training, mainly learns to leave inventory untouched\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_5\n",
    "# 5 runs, i_0 = 400, oder_size=100, execution action space, some runs with only one convergence to optimal policy, trunc at 7, expl 500 epis, eps_start 0.8, eps_end 0.02, lr 1e-4 to 1e-6 in 1000 epis , epis 1000, seeds 1,2,3,5,7 , quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "# only run 2 learned an acceptable policy, number of episodes to little, reduction from high exploration to low to fast\n",
    "\n",
    "# INVALID â€“ RUN AGAIN\n",
    "# custom_execution_v0_training_no_dampening_6\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, some runs with convergence to suboptimal policy, trunc at 7, expl 1000 epis, eps_start 0.8, eps_end 0.02, lr 1e-4 to 1e-6 in 2000 epis , epis 2000, seeds 5,7 , quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "# both runs a bit suboptimal, first reduces too much end in [-200, 0], second also a little too much but good\n",
    "\n",
    "# INVALID - RUN AGAIN \n",
    "# custom_execution_v0_training_no_dampening_7\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, some runs with ?? convergence to optimal policy, trunc at 7, expl 1000 epis, eps_start 0.8, eps_end 0.02, lr 1e-4 to 1e-6 in 2000 epis , epis 2000, seeds 5,7 , quadratic penalty C=-1, basepoint=T, discount=.99, damp=0.\n",
    "# run 0 it learned a suboptimal policy, run a nearly optimal\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_8\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, one run with convergence to optimal policy, trunc at 7, expl 1000 epis, eps_start 0.8, eps_end 0.02, lr exp decay from 1e-3 with 0.995 , epis 2000, seeds 7,5 , quadratic penalty C=-1, basepoint=T, discount=.99, damp=0. \n",
    "# first runs optimal, second run shit\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_9\n",
    "# 5 runs, i_0 = 400, oder_size=100, execution action space, no run with convergence to optimal policy, trunc at 7, expl 1000 epis, eps_start 0.8, eps_end 0.02, lr exp decay from 1e-3 with 0.995 , epis 2000, seeds 1,2,3,5,7 , quadratic penalty C=-1, basepoint=T, discount=.99, damp=0. \n",
    "# shitty shit shit, why?\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_10\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with some convergence to optimal policy, trunc at 7, expl 1000 epis, eps_start 0.5, eps_end 0.02, lr exp decay from 1e-4 with 0.997 , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=1., damp=0. \n",
    "# first run did not learn anything, second suboptimal reduces only to 200 high variance, lr decay too fast\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_11\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with some convergence to optimal policy, trunc at 7, expl 1000 epis, eps_start 0.5, eps_end 0.02, lr episode exp decay from 1e-4 with 0.998 , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=1., damp=0. \n",
    "# first run did not learn much reduces only 300, second suboptimal reduces reduces a bit too much to -100\n",
    "\n",
    "# ------ -------\n",
    "# custom_execution_v0_training_no_dampening_12\n",
    "# eps_start = 0.8 eps_end = 0.02, discount = 1.0\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, convergence to almost optimal policy, trunc at 7, expl 1000 epis, eps_start 0.8, eps_end 0.02, lr 1e-4 to 1e-7 in 2000 epis , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "# both runs convergence to almost optimal policy, 2nd a bit better, both overshoot a little at beginning then buy again\n",
    "# for intermediate -3, testing was even slightly better\n",
    "# first Q values converge to mean reward with reward window [-200:0]\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_13\n",
    "# eps_start = 0.8 eps_end = 0.02, discount = 0.99\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, convergence optimal policy, trunc at 7, expl 1000 epis, eps_start 0.8, eps_end 0.02, lr 1e-4 to 1e-7 in 2000 epis , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=.99, damp=0.\n",
    "# even a better than _12, it does not overshoot reduction of inventory, range of terminal inventory is smaller\n",
    "# first run sometimes sells at end of episode, second run very stable \n",
    "# for intermediate -3 testing run_1 was better, run_2 sells to much at episode end, before would be really good\n",
    "# first Q converges to value a bit below mean reward for both runs\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_14\n",
    "# eps_start = 0.8 eps_end = 0.01, discount = 0.99\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, convergence optimal policy, trunc at 7, expl 1000 epis, eps_start 0.8, eps_end 0.01, lr 1e-4 to 1e-7 in 2000 epis , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=.99, damp=0.\n",
    "# second run is really good, first run only reduces down to ~200 unfortunately \n",
    "\n",
    "# custom_execution_v0_training_no_dampening_15\n",
    "# eps_start = 0.8 eps_end = 0.01, discount = 1.0\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, convergence optimal policy, trunc at 7, expl 1000 epis, eps_start 0.8, eps_end 0.01, lr 1e-4 to 1e-7 in 2000 epis , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=1.0, damp=0.\n",
    "# First run learned nothing, reduces way too much than overshoots buying. Second run ok but in range [0, -200]\n",
    "\n",
    "# ------ with eps start 0.5 ------\n",
    "# custom_execution_v0_training_no_dampening_16\n",
    "# eps_start = 0.5, eps_end = 0.02, discount = 1.0\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with no convergence to optimal policy, trunc at 7, expl 1000 epis, eps_start 0.5, eps_end 0.02, lr 1e-4 to 1e-7 in 2000 epis , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "# no run did learn a good Q estimate\n",
    "# first Q values did not converge properly\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_17\n",
    "# eps_start = 0.5, eps_end = 0.02, discount = 0.99\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, no convergence to optimal policy, trunc at 7, expl 1000 epis, eps_start 0.5, eps_end 0.02, lr 1e-4 to 1e-7 in 2000 epis , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=.99, damp=0.\n",
    "# first run failed to converge, highly unstable in training, second run learned suboptimal policy, that reduces to 300 and only at end to 200\n",
    "# first Q values did not converge\n",
    "\n",
    "# ------ with decay mode 'steps' ------\n",
    "# custom_execution_v0_training_no_dampening_18\n",
    "# eps_start = 0.5, eps_end = 0.02, discount = 1.0, linear step decay\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with ?? convergence to optimal policy, trunc at 7, expl 1000*181 steps, eps_start 0.5, eps_end 0.02, lr 1e-4 to 1e-7 in 2000*181 steps , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "# 1st run suboptimal, reduced inventory but to a range [-100,100] and overshoot in buying and selling at beginning\n",
    "# 2nd run did not learn anything useful\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_19\n",
    "# eps_start = 0.8, eps_end = 0.02, discount = 1.0, linear step decay\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with ?? convergence to optimal policy, trunc at 7, expl 1000*181 steps, eps_start 0.8, eps_end 0.02, lr 1e-4 to 1e-7 in 2000*181 steps , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "# 1st run did not learn good policy, reduced halfway through the episode not stable\n",
    "# 2nd run better, overshoots a little, intermediate at -31 is perfect\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_20\n",
    "# eps_start = 1.0, eps_end = 0.02, discount = 1.0, linear step decay\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with some convergence to optimal policy, trunc at 7, expl 1000*181 steps, eps_start 1.0, eps_end 0.02, lr 1e-4 to 1e-7 in 2000*181 steps , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "# 1st run suboptimal, wide range of terminal inventory on [-100,100]\n",
    "# 2nd run learned good policy, but not particularly better than other cases with episode decay\n",
    "# BUT perfect convergence of 1st Q value to mean reward !\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_20a\n",
    "# eps_start = 1.0, eps_end = 0.02, discount = 1.0, linear step decay, order size = 50\n",
    "# 2 runs, i_0 = 400, oder_size=50, execution action space, with some convergence to optimal policy, trunc at 7, expl 1000*181 steps, eps_start 1.0, eps_end 0.02, lr 1e-4 to 1e-7 in 2000*181 steps , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "# ADD DESCRIPTION\n",
    "\n",
    "# TO RUN\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_21\n",
    "# eps_start = 1.0, eps_end = 0.02, discount = 1.0, linear step decay, lr start 5*1e-5\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with ?? convergence to optimal policy, trunc at 7, expl 1000*181 steps, eps_start 1.0, eps_end 0.02, lr 5*1e-5 to 1e-7 in 2000*181 steps , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_22\n",
    "# eps_start = 1.0, eps_end = 0.02, discount = 1.0, exp step decay\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with ?? convergence to optimal policy, trunc at 7, expl 1000*181 steps, eps_start 1.0, eps_end 0.02, lr exp step decay from 1e-4 with 0.999985 , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_23\n",
    "# eps_start = 0.5, eps_end = 0.02, discount = 0.99\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with ?? convergence to optimal policy, trunc at 7, expl 1000*181 steps, eps_start 0.5, eps_end 0.02, lr 1e-4 to 1e-7 in 2000*181 steps , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=.99, damp=0.\n",
    "# NO NEED FOR THIS ONE\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_24\n",
    "# eps_start = 0.8, eps_end = 0.02, discount = 0.99\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with ?? convergence to optimal policy, trunc at 7, expl 1000*181 steps, eps_start 0.8, eps_end 0.02, lr 1e-4 to 1e-7 in 2000*181 steps , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=.99, damp=0.\n",
    "\n",
    "# -------------\n",
    "# custom_execution_v0_training_no_dampening_100\n",
    "# eps_start = 0.8, eps_end = 0.02, discount = 1.0, linear step decay, order size = 50, current_inv as reward\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with ?? convergence to optimal policy, trunc at 7, expl 1000*181 steps, eps_start 1.0, eps_end 0.02, lr 1e-4 to 1e-7 in 2000*181 steps , epis 2000, seeds 1,2 , quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "# ADD DESCRIPTION\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_101\n",
    "# eps_start = 0.8 eps_end = 0.02, discount = 1.0, linear episode decay, current_inv as reward\n",
    "# 1 run, i_0 = 400, oder_size=100, execution action space, with ?? convergence to almost optimal policy, trunc at 7, expl 1000 epis, eps_start 0.8, eps_end 0.02, lr 1e-4 to 1e-7 in 2000 epis , epis 2000, seeds 1, quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "# ADD DESCRIPTION\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_102\n",
    "# eps_start = 0.8 eps_end = 0.02, discount = 1.0, linear episode decay, previous_inv as reward with linear terminal reward\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with some convergence to optimal policy, trunc at 7, expl 1000 epis, eps_start 0.8, eps_end 0.02, lr 1e-4 to 1e-7 in 2000 epis , epis 2000, seeds 1, quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "# 1st run learned nothing, 2nd run close to optimal, narrow range, similar to runs with same setup but quadratic terminal reward\n",
    "# 3rd run with seed = 3 is running # ADD DECRIPTION\n",
    "\n",
    "# custom_execution_v0_training_no_dampening_103\n",
    "# eps_start = 0.8 eps_end = 0.02, discount = 1.0, linear episode decay, current as reward with flat terminal reward and 0 exploration\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with some convergence to optimal policy, trunc at 7, expl 1000 epis, eps_start 0.8, eps_end 0.0, lr 1e-4 to 1e-7 in 2000 epis , epis 2000, seeds 1, quadratic penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "# both runs learned a solid policy, however both only reduced to the range [50, 100] and not furhter\n",
    "# -> not saved\n",
    "\n",
    "# ************************************************************************************************************************************************************************************************************************************************************************************************************\n",
    "# custom_execution_v0_training_no_dampening_104\n",
    "# eps_start = 0.8, eps_mid=0.02, eps_end = 0.0, discount = 1.0, linear episode decay, previous as reward with flat terminal reward\n",
    "# 2 runs, i_0 = 400, oder_size=100, execution action space, with ?? convergence to optimal policy, trunc at 7, expl mixed linear 2000 epis, eps_start 0.8, eps_mid=0.02 eps_end 0.0, lr 1e-4 to 1e-7 in 2000 epis , epis 2000, seeds 2,3, flat terminal penalty C=-1, basepoint=T, discount=1., damp=0.\n",
    "\n",
    "\n",
    "# TO TRY:\n",
    "# start with i_0=1400, i_max=2000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training results\n",
    "\n",
    "### Plot single training run results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select training run number for plotting\n",
    "run_id = 0\n",
    "results = training_results[run_id]\n",
    "\n",
    "utils.plot_results([\n",
    "    results[\"rewards\"],\n",
    "    results[\"losses\"],\n",
    "    results[\"cash\"],\n",
    "    results[\"terminal_inventory\"],\n",
    "], size=(6,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot single episode observation / action history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 0\n",
    "episode_id = -2\n",
    "env = generate_env()\n",
    "\n",
    "observation_history = list(training_results[run_id][\"observations\"][episode_id])\n",
    "action_history = training_results[run_id][\"actions\"][episode_id]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6,3))\n",
    "ax[0].plot(observation_history)\n",
    "ax[0].set_xlabel(f\"Observation history in episode {episode_id}\")\n",
    "ax[0].legend([\"Time pct\", \"Inventory pct\"], loc=\"best\")\n",
    "\n",
    "ax[1].plot(action_history)\n",
    "ax[1].set_xlabel(f\"Action history in episode {episode_id}\")\n",
    "actions = [*range(env.action_space.n)]\n",
    "ax[1].set_yticks(actions, labels=[f\"A{i}\" for i in actions])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "keys_of_interest = [\"rewards\", \"losses\", \"terminal_inventory\"]\n",
    "mean_dict = {}\n",
    "std_dict = {}\n",
    "\n",
    "# compute mean and standard deviation for keys of interest\n",
    "for key in keys_of_interest:\n",
    "    values = [run[key] for run in training_results.values()]\n",
    "    mean_dict[key] = np.mean(values, axis=0)\n",
    "    std_dict[key] = np.std(values, axis=0)\n",
    "\n",
    "# names for plot titles\n",
    "plot_names = [\"reward\", \"loss\", \"terminal inventory\"]\n",
    "plot_names_dict = {key: name for key, name in zip(keys_of_interest, plot_names)}\n",
    "\n",
    "# gernate mean + std plots\n",
    "for key in keys_of_interest:\n",
    "    plt.plot(mean_dict[key], color=\"b\", label=\"mean \" + plot_names_dict[key])\n",
    "    plt.fill_between(\n",
    "        range(len(mean_dict[key])),\n",
    "        mean_dict[key] - 1 * std_dict[key],\n",
    "        mean_dict[key] + 1 * std_dict[key],\n",
    "        color=\"b\", alpha=0.2, label=\"+/- standard deviation\"\n",
    "    )\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.title(\"Average {} per episode\".format(plot_names_dict[key]))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "To load prior saved training results, set `load_training_results_flag` to True and paste trainings results' file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_training_results_flag = False\n",
    "\n",
    "if load_training_results_flag:\n",
    "    # load training results\n",
    "    with open('../results/custom_execution_v0_training_no_dampening_20240912_B.pkl', 'rb') as f:        \n",
    "        training_results = pickle.load(f)\n",
    "        print(\"*** training results loaded ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a learned Q-function estimate\n",
    "\n",
    "We test the Q-functions learning in training. If an intermediate Q-function estimate saved at some checkpoint instead of the last one for each training run should be tested, set `checkpoint_policy_flag` to True and select a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "checkpoint_policy_flag = True\n",
    "checkpoint = -1\n",
    "\n",
    "truncation_order = 7\n",
    "n_runs = len(training_results)\n",
    "n_test_episodes = 5\n",
    "test_results = {run : [] for run in range(n_runs)}\n",
    "\n",
    "seeds = utils.generate_prime_seeds(100)\n",
    "\n",
    "runs_pbar = tqdm(test_results.keys())\n",
    "for run in runs_pbar:\n",
    "    runs_pbar.set_description(f\"Run\")\n",
    "\n",
    "    env = generate_env(random.choice(seeds))\n",
    "    sigpolicy = SigQFunction(env, **sigq_params)\n",
    "    \n",
    "    if checkpoint_policy_flag:\n",
    "        sigpolicy.load_state_dict(training_results[run][\"intermediate\"][checkpoint])\n",
    "    else:\n",
    "        sigpolicy.load_state_dict(final_Q_functions[run])\n",
    "    \n",
    "    sigpolicy.eval()\n",
    "    with torch.no_grad():\n",
    "        test_results[run] = test(env, sigpolicy, n_test_episodes, epsilon=0., debug_mode=\"info\")\n",
    "    del(sigpolicy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 0\n",
    "episode_id = None\n",
    "test_run = test_results[run_id]\n",
    "\n",
    "rolling_avg_flag = False\n",
    "rolling_avg_window = 0\n",
    "\n",
    "names = [\"rewards\", \"terminal_inventories\", \"actions\", \"inventories\"]\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, sharex=False)\n",
    "for ax, id in zip(axes.flat, range(4)):\n",
    "    ax.set_title(names[id] if id < 2 else names[id] + (\n",
    "                     \" in episode \" + str(episode_id) if episode_id != -1 else \" in last episode\")\n",
    "    )\n",
    "    if episode_id is not None:\n",
    "        ax.plot(test_run[names[id]] if id < 2 else test_run[names[id]][episode_id])\n",
    "    else:\n",
    "        ax.plot(test_run[names[id]]) if id < 2 else [ax.plot(x) for x in test_run[names[id]]]\n",
    "\n",
    "    if rolling_avg_flag:\n",
    "        ax.plot(utils.moving_average(\n",
    "                test_run[names[id]] if id < 2 else test_run[names[id]][episode_id], rolling_avg_window\n",
    "            ))\n",
    "    ax.set_xlabel(\"Episodes\" if id < 2 else \"Steps\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"\"\"       |   reward: mean, std   |   inventory: mean, std   |    cash: mean, std \"\"\")\n",
    "for i in range(len(test_results)):\n",
    "    print(\n",
    "        \"\"\"run: {0} |    {1:0.4f}, {2:0.4f}    |    {3:0.4f}, {4:0.4f}    |    \"\"\".format(\n",
    "            i, \n",
    "            np.mean(test_results[i][\"rewards\"]), np.std(test_results[i][\"rewards\"]),\n",
    "            np.mean(test_results[i][\"terminal_inventories\"]), np.std(test_results[i][\"terminal_inventories\"])),\n",
    "            np.mean(test_results[i][\"cash\"])/100, np.std(test_results[i][\"cash\"])/np.sqrt(100)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE test results\n",
    "with open('../results/test_results_NEW.pkl', 'wb') as f:  \n",
    "    pickle.dump(results, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check first Q-value convergence\n",
    "\n",
    "**Note**: We need to compare this with the average episode reward once all runs are performed. Also longer runs might be needed since I expect the average episode reward to be above the Q values reached with 250 episodes \n",
    "\n",
    "### With intermediate Q functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "run_id = 0\n",
    "mean_epis = 100\n",
    "\n",
    "observation_interval_flag = True if env.observation_interval > 0 else False\n",
    "print(\"observe first interval: {}\".format(observation_interval_flag))\n",
    "\n",
    "basepoint_flag = True\n",
    "print(\"basepoint: {}\".format(basepoint_flag))\n",
    "\n",
    "first_Q_values = []\n",
    "policy = SigQFunction(env, truncation_order)\n",
    "\n",
    "if observation_interval_flag:\n",
    "    # steps to only observe\n",
    "    do_nothing_steps = max(1, math.floor(env.observation_interval / env.timestep_duration))\n",
    "    first_history = list(training_results[run_id][\"observations\"][0])[0:do_nothing_steps+1]\n",
    "else:\n",
    "    # no observation before first action\n",
    "    first_history = list(training_results[run_id][\"observations\"][0])[0:2]\n",
    "first_history = torch.tensor(\n",
    "    first_history, requires_grad=False, dtype=torch.float\n",
    ").unsqueeze(0)\n",
    "\n",
    "for state_dict in training_results[run_id][\"intermediate\"]: \n",
    "    policy.load_state_dict(state_dict)    \n",
    "    policy.eval()\n",
    "    sig = policy.compute_signature(first_history, basepoint=basepoint_flag)  \n",
    "    first_Q_values.append(policy(sig)[0].detach())\n",
    "    policy.train()     \n",
    "\n",
    "first_Q_values = torch.stack(first_Q_values, dim=0)\n",
    "plt.plot(first_Q_values)\n",
    "mean_reward = torch.tensor(training_results[run_id][\"rewards\"][-mean_epis:]).mean()\n",
    "plt.plot([mean_reward for x in range(len(first_Q_values))])\n",
    "plt.legend([\"Action \" + str(i) for i in range(first_Q_values.shape[-1])],\n",
    "           loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "first_Q_mean = first_Q_values.mean(dim=-1)\n",
    "plt.plot(first_Q_mean)\n",
    "mean_reward = torch.tensor(training_results[run_id][\"rewards\"][-mean_epis:]).mean()\n",
    "plt.plot([mean_reward for x in range(len(first_Q_mean))])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With saved Q-values\n",
    "\n",
    "This section currently only works for training run `results_v2_copy_1214_2` and `results_v2_copy_1215_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values = list(map(torch.detach, results[\"first_Q_values\"]))\n",
    "Q_values = [\n",
    "    torch.stack(Q_values[i:i+181])\n",
    "    for i in range(0, len(Q_values)-1, 181)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_id = -1\n",
    "plt.plot(Q_values[episode_id])\n",
    "plt.legend([\"Action \" + str(i) for i in range(Q_values[episode_id].shape[-1])],\n",
    "           loc=\"upper left\")\n",
    "plt.title(\"Q values in episode {}\".format(episode_id))\n",
    "plt.show()\n",
    "\n",
    "first_Q_values = torch.stack([x[0] for x in Q_values], dim=0)\n",
    "mean_reward = torch.tensor(results[\"rewards\"][-300:]).mean()\n",
    "\n",
    "plt.plot(first_Q_values)\n",
    "plt.plot([mean_reward for x in range(len(first_Q_values))])\n",
    "plt.legend([\n",
    "        \"Action \" + str(i) \n",
    "        if i < first_Q_values.shape[-1] else \"Mean reward\" \n",
    "        for i in range(first_Q_values.shape[-1]+1)\n",
    "    ],\n",
    "    loc=\"upper right\")\n",
    "plt.title(\"First Q values per episode\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
