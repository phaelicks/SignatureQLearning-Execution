{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run base.ipynb\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import scipy.stats as stats \n",
    "\n",
    "from abides_gym_custom_execution_environment import SubGymMarketsCustomExecutionEnv\n",
    "from sigqlearning_qfunctions import SigQFunction\n",
    "from sigqlearning_test_execution import test\n",
    "from sigqlearning_train_execution import train\n",
    "from sigqlearning_baseline_execution import run_baseline\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment as gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register execution env for gym use \n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id=\"custom-execution-v0\",\n",
    "    entry_point=SubGymMarketsCustomExecutionEnv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to generate environment with specified parameters\n",
    "def generate_env(seed=None, **env_params):\n",
    "    \"\"\"\n",
    "    generates specific environment with the parameters defined and set the seed\n",
    "    \"\"\"\n",
    "    env = gym.make(id = \"custom-execution-v0\", **env_params)\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment parameters\n",
    "env_params = dict(\n",
    "    background_config = \"rmsc04\",\n",
    "    mkt_close = \"10:05:00\",\n",
    "    timestep_duration = \"10s\",\n",
    "    first_interval = \"00:05:00\",\n",
    "    observation_interval = \"00:00:00\",            \n",
    "    order_fixed_size = 50,\n",
    "    max_inventory = 1000,\n",
    "    starting_inventory = 700,\n",
    "    terminal_inventory_reward = -0.7, # reward or penalty\n",
    "    terminal_inventory_mode = 'quadratic', # quadratic, linear, flat\n",
    "    running_inventory_reward_dampener = 0., # 0.6, 1.0\n",
    "    damp_mode = None, # asymmetric\n",
    "    debug_mode = False,\n",
    "    reward_multiplier = 'quadratic_positive', # running reward mode\n",
    "    reward_multiplier_float = None, #1.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline policy\n",
    "\n",
    "The baseline policy sells its inventory as long as the absolute inventory value is above 5% of the maximal inventory and afterwards stops trading until the end of the episode. It is exactly the policy the RL agent is supposed to learn and offers a baseline reward to compare it against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run baseline\n",
    "env = generate_env(1000, **env_params) # seed different from any test run seed\n",
    "baseline_results_dict = run_baseline(env, episodes=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_load_baseline = 'load'\n",
    "date_time_id = '20250127_A'\n",
    "\n",
    "# save or load\n",
    "if save_load_baseline == 'save':\n",
    "    file_path = '../results/custom_execution_baseline_results_{}_NEW.pkl'.format(date_time_id)\n",
    "    with open(file_path, 'wb') as fout:  \n",
    "        pickle.dump(baseline_results_dict, fout)\n",
    "    print(f\"***** baseline results SAVED under {file_path} *****\")\n",
    "\n",
    "if save_load_baseline == 'load':\n",
    "    file_path = '../results/custom_execution_baseline_results_{}.pkl'.format(date_time_id)\n",
    "    with open(file_path, 'rb') as fin:        \n",
    "        baseline_results_dict = pickle.load(fin)\n",
    "        print(f\"***** baseline results LOADED from {file_path} *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_stats = []\n",
    "baseline_stats.append(np.mean(baseline_results_dict['rewards'])) # mean reward\n",
    "baseline_stats.append(np.std(baseline_results_dict['rewards'])) # std reward\n",
    "baseline_stats.append(np.median(baseline_results_dict['rewards'])) # median reward\n",
    "baseline_stats.append(np.mean(baseline_results_dict['terminal_inventories'])) # mean terminal inventory\n",
    "baseline_stats.append(np.std(baseline_results_dict['terminal_inventories'])) # std terminal inventory\n",
    "baseline_stats.append(int(np.min(baseline_results_dict['terminal_inventories']))) # min terminal inventory\n",
    "baseline_stats.append(int(np.max(baseline_results_dict['terminal_inventories']))) # max terminal inventory\n",
    "    \n",
    "columns = [\n",
    "    'Mean\\nreward', 'Std\\nreward', 'Median\\nreward', 'Mean terminal\\ninventory', \n",
    "    'Std terminal\\ninventory', 'Min terminal\\ninventory', 'Max terminal\\ninventory' \n",
    "]\n",
    "print(tabulate([baseline_stats], headers=columns, tablefmt='fancy_grid', floatfmt='.5f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_id = -1\n",
    "utils.plot_baseline_results(baseline_results_dict, episode_id, ma_window=20, figsize=(6, 4))\n",
    "\n",
    "save_baseline_plot = False\n",
    "if save_baseline_plot:\n",
    "    utils.save_baseline_trajectories(baseline_results_dict, episode_ids=[-6, -9], show=True, file_path=\"../figures/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence interval baseline rewards\n",
    "\n",
    "We inspect the distribution of rewards from the baseline run and find that it is moderately right-skewed. We then compute confidence intervals for the mean reward using three different methods: \n",
    "1. Normal-based CI (Large Sample Theory),   \n",
    "2. Bootstrap CI,\n",
    "3. Log-Transformed CI.\n",
    "\n",
    "The log-transformed CI is computed by fitting a lognormal distribution to the negative rewards and following the approach for confidence intervals for three parameter log-normal distrobutions in Olsson (2017). The obtained CI are then transformed back to the original scale. The bootstrap CI is obtained by resampling the rewards with replacement and computing the mean of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rewards = np.array(baseline_results_dict['rewards'])\n",
    "\n",
    "# histogram and boxplot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(11,3.5))\n",
    "axs[0].hist(baseline_rewards, bins=80, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axs[0].set_title('Baseline rewards histogram', fontsize=11)\n",
    "axs[0].set_xlabel('Rewards')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "bp = axs[1].boxplot(baseline_rewards, showmeans=True, meanline=True, patch_artist=True, \n",
    "                    boxprops=dict(facecolor=\"white\"))\n",
    "axs[1].legend([bp['means'][0], bp['medians'][0], bp['boxes'][0], bp['whiskers'][0]], \n",
    "              ['mean', 'median', 'IQR', r'$\\pm$1.5 IQR'], loc='lower right')\n",
    "axs[1].set_ylabel('Rewards')\n",
    "axs[1].set_xticks([1], [])\n",
    "axs[1].set_title('Baseline rewards boxplot', fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rewards = np.array(baseline_results_dict['rewards']) # Ensure the data is a NumPy array\n",
    "\n",
    "alpha = 0.05 # significance level 1 - alpha\n",
    "n = len(baseline_rewards)\n",
    "t_critical = stats.t.ppf(1 - alpha / 2, df=n-1)  # 95% confidence level\n",
    "\n",
    "# compute skewness\n",
    "sample_skewness = stats.skew(baseline_rewards)\n",
    "print('Reward distribution skewness:', round(sample_skewness, 5))\n",
    "\n",
    "# 1. Normal-based CI (Large Sample Theory)\n",
    "sample_mean = np.mean(baseline_rewards)\n",
    "sample_std = np.std(baseline_rewards, ddof=1)\n",
    "print('standard error:', sample_std / np.sqrt(n))\n",
    "\n",
    "normal_CI = (sample_mean - t_critical * sample_std / np.sqrt(n), \n",
    "             sample_mean + t_critical * sample_std / np.sqrt(n))\n",
    "\n",
    "# 2. Bootstrap CI\n",
    "np.random.seed(1) \n",
    "bootstrap_samples = np.random.choice(baseline_rewards, size=(1000, n), replace=True)\n",
    "bootstrap_means = np.mean(bootstrap_samples, axis=1)\n",
    "bootstrap_CI = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "# 3. Log-Transformed CI\n",
    "shape, loc, scale = stats.lognorm.fit(-baseline_rewards, loc=0.0)\n",
    "stdized_rewards = (np.log(-baseline_rewards - loc) - np.log(scale)) / shape\n",
    "fitted_mean = np.log(scale) # mean of (- rewards - loc), lognormal with loc=0\n",
    "fitted_std_estimate = np.sqrt(shape ** 2 / n + shape ** 4 / (2*(n - 1))) # based on Olsson paper\n",
    "\n",
    "lognorm_CI = (np.exp(fitted_mean + shape ** 2 / 2 - t_critical * fitted_std_estimate), \n",
    "              np.exp(fitted_mean + shape ** 2 / 2 + t_critical * fitted_std_estimate))\n",
    "lognorm_CI = (-lognorm_CI[1] - loc, -lognorm_CI[0] - loc)\n",
    "\n",
    "# print results\n",
    "lognorm_params = np.array([shape, loc, scale])\n",
    "print(tabulate([lognorm_params], headers=['shape', 'loc', 'scale'], showindex=['lognormal fit'],\n",
    "               tablefmt='fancy_grid', floatfmt='.5f'))\n",
    "\n",
    "CI_results = [normal_CI, lognorm_CI, bootstrap_CI]\n",
    "print(tabulate(CI_results, headers=[f'{int(100*(1-alpha))}% Confidence intervals', 'lower bound', 'upper bound'], \n",
    "               showindex=['normal CI', 'lognormal CI', 'bootstrap CI'], tablefmt='fancy_grid', floatfmt='.6f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature parameters\n",
    "sigq_params = dict(\n",
    "    sig_depth = 7,\n",
    "    basepoint = [0., 0.], \n",
    "    initial_bias = 0.01,\n",
    ")\n",
    "\n",
    "# training parameter\n",
    "training_params = dict(\n",
    "    episodes = 3000,\n",
    "    discount = 1.0,\n",
    "    learning_rate = 5*1e-5,\n",
    "    learning_rate_decay = dict(mode='exponential', factor=0.999),\n",
    "        #dict(mode=None),\n",
    "        #dict(mode='linear', end_value=1e-7, epochs=2000), \n",
    "    exploration = 'greedy',\n",
    "    epsilon = 1,\n",
    "    epsilon_decay = dict(mode='exponential', factor=0.997),\n",
    "        #dict(mode=None),\n",
    "        #dict(mode='linear', end_value=0.02, epochs=1000),  \n",
    "    decay_mode = 'episodes',\n",
    "    debug_mode = None,\n",
    "    progress_display = 'livelossplot',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn a Q-function estimate\n",
    "Learn a Q-function estimate with Signature-Q-learning. We run 5 different seeds.\n",
    "\n",
    "**Note**: This code cell takes a long time to run. Would recommend to copy this cell in a python script and let it run in the back with [screen](https://linuxize.com/post/how-to-use-linux-screen/) or [tmux](https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/) for instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training runs\n",
    "n_runs = 10\n",
    "\n",
    "# dict to store results\n",
    "training_results_dict = {run : [] for run in range(n_runs)}\n",
    "training_seeds = {\n",
    "    run : seed for run, seed \n",
    "    in zip(training_results_dict, utils.generate_prime_seeds(n_runs, random=True))\n",
    "}\n",
    "\n",
    "final_Q_functions = {}\n",
    "\n",
    "# training runs\n",
    "runs_pbar = tqdm(training_results_dict.keys(), desc='Training runs')\n",
    "for run in runs_pbar:\n",
    "    env = generate_env(training_seeds[run], **env_params)\n",
    "    sigqfunction = SigQFunction(env, **sigq_params)\n",
    "    training_results_dict[run] = train(env, sigqfunction, **training_params)\n",
    "    final_Q_functions[run] = sigqfunction.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save training results\n",
    "\n",
    "**Note**: Training results are saved under `../results/file_name + today + id`, where `today` is the current date\n",
    "in the form `%Y%M%d` and `id` an upper case letter to identifiy multiple files saved on the same date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"custom_execution_training\"\n",
    "results_dict=dict(training_results=training_results_dict, final_Q_functions=final_Q_functions,\n",
    "                  sig_params=sigq_params, training_params=training_params, env_params=env_params,\n",
    "                  training_seeds=training_seeds)\n",
    "date_time_id = utils.save_results(results_dict, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training plots\n",
    "\n",
    "### Single training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select training run number for plotting\n",
    "run_id = 3\n",
    "start = -2000\n",
    "end = -1\n",
    "utils.plot_training_run_results(training_results_dict, run_id, figsize=None)\n",
    "plt.plot(training_results_dict[run_id]['terminal_inventory'][start:end])\n",
    "plt.plot([0 for _ in range(end - start)], color='black')\n",
    "plt.plot([100 for _ in range(end - start)], color='black')\n",
    "plt.plot([-100 for _ in range(end - start)], color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaged training results\n",
    "\n",
    "And save each plot as single file in `..\\figures`. Each file name contains `date_time_id` of the corresponding file containing saved training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plots = False\n",
    "if save_plots: \n",
    "    utils.save_mean_training_results_plots(training_results_dict, date_time_id=date_time_id, \n",
    "                                           file_name='custom_execution', show=False)\n",
    "\n",
    "utils.plot_mean_training_results(training_results_dict, figsize=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation-action histories\n",
    "\n",
    "To gain a more qualitative insight into the policies based on the learned Q-functions, we plot the observation and action trajectories of specific training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ids = 'all'\n",
    "episode_ids = [0, 999, 1999, 2999] \n",
    "utils.plot_inventory_action_histories(training_results_dict, run_ids, episode_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First observation value convergence\n",
    "\n",
    "**Note**: We need to compare trajectory of the first observation value as provided by the Q-function approximations during training, with the average episode reward once all runs are performed. If the reward and the first observation value converge towards the same value, this is an indication that the algorithm has converged. Note, however, that this does not necessariliy mean that the Q-function approximation has converged to the true optimal Q-functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot = False\n",
    "utils.plot_first_observation_values(training_results_dict, run_ids='all', mean=True, \n",
    "                                    std=True, figsize=(8,3.2), line=-0.0844,\n",
    "                                    save=save_plot, date_time_id='20250127_A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot = False\n",
    "utils.plot_reward_vs_first_obs_value(training_results_dict, episode_window=(-1000,-1), \n",
    "                                     figsize=(8,3), line=-0.0844,\n",
    "                                     save=save_plot, date_time_id='20250127_A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "To load prior saved training results, set `load_training_results_flag` to True and paste trainings results' file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_training_results_flag = True\n",
    "from pprint import pp as pprint\n",
    "\n",
    "if load_training_results_flag:\n",
    "    date_time_id = '20250127_A'\n",
    "    file_path = '../results/custom_execution_training_{}.pkl'.format(date_time_id)\n",
    "    # load training results\n",
    "    with open(file_path, 'rb') as fin:        \n",
    "        loaded_training_dict = pickle.load(fin)\n",
    "\n",
    "    env_params = loaded_training_dict[\"env_params\"]\n",
    "    sigq_params = loaded_training_dict[\"sig_params\"]\n",
    "    training_params = loaded_training_dict[\"training_params\"]\n",
    "    training_results_dict = loaded_training_dict[\"training_results\"]\n",
    "    final_Q_functions = loaded_training_dict[\"final_Q_functions\"]\n",
    "    training_seeds = loaded_training_dict[\"training_seeds\"]\n",
    "    n_runs = len(training_results_dict)\n",
    "\n",
    "    print(\"***** training results {} loaded *****\".format(date_time_id))\n",
    "    print('Performed training runs: {}. With parameters:\\n'.format(n_runs))\n",
    "    pprint({key: value for key, value in loaded_training_dict.items() if key not in ('training_results', 'final_Q_functions')})\n",
    "    del loaded_training_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Q-function estimate\n",
    "\n",
    "We test the Q-functions learning in training. If an intermediate Q-function estimate saved at some checkpoint instead of the last one for each training run should be tested, set `checkpoint_policy_flag` to True and select a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = -1 # final\n",
    "\n",
    "n_test_episodes = 2000\n",
    "test_results_dict = {}\n",
    "test_seeds = utils.generate_prime_seeds(100, random=True)\n",
    "\n",
    "runs_pbar = tqdm(training_results_dict.keys(), desc='Test runs')\n",
    "for run in runs_pbar:\n",
    "    env = generate_env(test_seeds[run], **env_params)\n",
    "    sigqfunction = SigQFunction(env, **sigq_params)\n",
    "    q_state_dict = final_Q_functions[run] if checkpoint == 'final' \\\n",
    "        else training_results_dict[run][\"intermediate\"][checkpoint]\n",
    "    sigqfunction.load_state_dict(q_state_dict)\n",
    "    test_results_dict[run] = test(env, sigqfunction, n_test_episodes, debug_mode=\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/load test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_or_load_flag = 'load'\n",
    "date_time_id = '20250127_A'\n",
    "\n",
    "if save_or_load_flag == 'save':\n",
    "    file_path = '../results/custom_execution_test_results_{}.pkl'.format(date_time_id)\n",
    "    with open(file_path, 'wb') as fout:  \n",
    "        pickle.dump((test_results_dict, test_seeds), fout)\n",
    "    print(f\"***** test results SAVED under {file_path} *****\")\n",
    "\n",
    "if save_or_load_flag == 'load':\n",
    "    file_path = '../results/custom_execution_test_results_{}.pkl'.format(date_time_id)\n",
    "    with open(file_path, 'rb') as fin:       \n",
    "        test_results_dict, test_seeds = pickle.load(fin)\n",
    "        #test_results_dict = pickle.load(fin)\n",
    "        print(f\"***** test results LOADED from {file_path} *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test statistics\n",
    "\n",
    "The following statistics are reported for each test run:\n",
    "- Mean reward and standard deviation\n",
    "- Mean terminal inventory and standard deviation\n",
    "- Minimum / maximum inventory in test run\n",
    "- Mean terminal cash and standard deviation\n",
    "- First observation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats = []\n",
    "rho = 50\n",
    "for test_run in test_results_dict.values():\n",
    "    run_stats = []\n",
    "    run_stats.append(np.mean(test_run['rewards'])) # mean reward\n",
    "    run_stats.append(np.std(test_run['rewards'])) # std reward\n",
    "    run_stats.append(np.median(test_run['rewards'])) # median reward\n",
    "    run_stats.append(np.mean(test_run['terminal_inventories'])) # mean terminal inventory\n",
    "    run_stats.append(np.std(test_run['terminal_inventories'])) # std terminal inventory\n",
    "    run_stats.append(int(np.min(test_run['terminal_inventories']))) # min terminal inventory\n",
    "    run_stats.append(int(np.max(test_run['terminal_inventories']))) # max terminal inventory\n",
    "    run_stats.append(sum(abs(np.array(test_run['terminal_inventories'])) <= rho//2) / \n",
    "                     len(test_run['terminal_inventories']) ) # pct in [-rho/2, rho/2]\n",
    "    run_stats.append(sum(abs(np.array(test_run['terminal_inventories'])) <= rho) / \n",
    "                     len(test_run['terminal_inventories']) ) # pct in [-rho, rho]\n",
    "    run_stats.append(test_run['first_obs_values'][-1]) # first observation value\n",
    "    test_stats.append(run_stats)\n",
    "\n",
    "columns = ['\\nRun', 'Mean\\nreward', 'Std\\nreward', 'Median\\nreward','Mean terminal\\ninventory', \n",
    "           'Std terminal\\ninventory', 'Min terminal\\ninventory', 'Max terminal\\ninventory', \n",
    "           'Pct in\\n[-rho/2,rho/2]', 'Pct in\\n[-rho,rho]', 'First obs\\nvalue']\n",
    "print(tabulate(test_stats, headers=columns, showindex=test_results_dict.keys(),tablefmt='simple', floatfmt='.5f'))\n",
    "print('mean of mean rewards:', round(np.mean([run_stats[0] for run_stats in test_stats]), 5))\n",
    "print('mean of median rewards:', round(np.mean([run_stats[2] for run_stats in test_stats]), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test plots\n",
    "\n",
    "### Single test run and episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 0\n",
    "episode_id = -1\n",
    "utils.plot_test_run_results(test_results_dict, run_id, episode_id, \n",
    "                            ma_window=10, figsize=(6, 4))\n",
    "\n",
    "for episode in test_results_dict[run_id]['inventories']:\n",
    "    plt.plot(episode)\n",
    "n_steps = 181\n",
    "plt.plot([50 for _ in range(n_steps)], color='black', linestyle='--')\n",
    "plt.plot([-50 for _ in range(n_steps)], color='black', linestyle='--')\n",
    "plt.plot([0 for _ in range(n_steps)], color='black', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectories of inventories for all test runs\n",
    "runs = list(range(10)) #[0, 1]\n",
    "n_steps = 181\n",
    "domain = range(-5, n_steps + 5)\n",
    "fig, axs = plt.subplots(5, 2, figsize=(10, 14), sharey=True, sharex=True)\n",
    "for ax, id_x in zip(axs.flatten(), runs):\n",
    "    for episode in test_results_dict[id_x]['inventories']:\n",
    "        ax.plot(episode)\n",
    "    for value in [-50, 0, 50]:\n",
    "        ax.plot(domain, [value for _ in domain], c='black', ls=':', lw=1.5)\n",
    "    ax.set_xlim(-5, n_steps + 5)\n",
    "    ax.set_ylim(-150, 750)\n",
    "    ax.set_title(f'Inventories in test run {id_x+1}') # for plotting + 1\n",
    "    if id_x % 2 == 0:\n",
    "        ax.set_ylabel(r'Inventory $i_t$')\n",
    "    if id_x in runs[-2:]:\n",
    "        ax.set_xlabel(r'Step $t$')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectories of actions for all test runs\n",
    "runs = list(range(10)) #[8, 9]\n",
    "n_steps = 181\n",
    "domain = range(-5, n_steps + 5)\n",
    "fig, axs = plt.subplots(5, 2, figsize=(10, 14), sharey=True, sharex=True)\n",
    "for ax, id_x in zip(axs.flatten(), runs):\n",
    "    for episode in test_results_dict[id_x]['actions']:\n",
    "        ax.plot(episode)\n",
    "    ax.set_xlim(-5, n_steps + 5)\n",
    "    ax.set_title(f'Actions in test run {id_x+1}') # for plotting + 1\n",
    "    if id_x % 2 == 0:\n",
    "        ax.set_ylabel(r'Action $a_t$')\n",
    "    if id_x in runs[-2:]:\n",
    "        ax.set_xlabel(r'Step $t$')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-plot of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figure = 0\n",
    "\n",
    "all_rewards = [run['rewards'] for run in test_results_dict.values()]\n",
    "all_rewards.append(baseline_results_dict['rewards'])\n",
    "plt.figure(figsize=(9, 4))\n",
    "bp = plt.boxplot(all_rewards, showmeans=True, meanline=True, patch_artist=True, \n",
    "                    boxprops=dict(facecolor=\"white\"),)# flierprops=dict(marker='o', markersize=5),)\n",
    "plt.plot([i for i in range(len(all_rewards)+2)], [0 for _ in range(len(all_rewards)+2)], \n",
    "         color='grey', ls=':', lw=1)\n",
    "plt.legend([bp['means'][0], bp['medians'][0], bp['boxes'][0], bp['whiskers'][0]], \n",
    "              ['mean', 'median', 'IQR', r'$\\pm$1.5 IQR'], loc='lower right', fontsize=9, ncol=2)\n",
    "\n",
    "plt.xlim(0.5, len(all_rewards)+0.5)\n",
    "plt.xticks(range(1, len(all_rewards)+1), ['Run {}'.format(i) for i in range(1, len(all_rewards))] + ['Baseline'])\n",
    "plt.ylabel('Reward')\n",
    "plt.tight_layout()\n",
    "if save_figure:\n",
    "    plt.savefig(f'../figures/testing_boxplot_all_rewards_{date_time_id}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline log-normal fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inspect the log-normal fit by visual means. No statistical goodness-of-fit test is performed, however, one could transform the data with the fitted paramters to the normal scale and test for normality with e.g. Anderson-Darling, Shapiro-Wilk, Jarque-Bera test.\n",
    "Note, that performing these tests with parameters fitted from the data changes the test value's dsitrbution under the null and p-values might be meaningless. \n",
    "\n",
    "Alternatively, parametric bootstrapping could be performed with a chosen test, to obtain the empirical distribution of the test value under the null distribution with the fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual inspection of log-normal fit\n",
    "samples = stats.lognorm.rvs(shape, loc, scale, size=n)\n",
    "stdzd_samples = (np.log(samples - loc) - np.log(scale)) / shape\n",
    "\n",
    "plt.figure(figsize=(12,8))    \n",
    "plt.boxplot([-baseline_rewards, samples], labels=['negative baseline rewards', 'log-normal samples'])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12,9))\n",
    "axs = axs.flatten()\n",
    "# axs 0\n",
    "axs[0].hist(samples, bins=80, color='blue', edgecolor='black', alpha=1)\n",
    "axs[0].hist(-baseline_rewards, bins=80, color='orange', edgecolor='black', alpha=0.8)\n",
    "axs[0].set_title('Histogram rewards and samples')\n",
    "axs[0].legend(['log-normal samples', 'negative baseline rewards'])\n",
    "# axs 1\n",
    "stats.probplot(-baseline_rewards, dist=stats.lognorm, sparams=(shape, loc, scale), plot=axs[1])\n",
    "axs[1].set_title('Q-Q plot negative rewards vs log-normal fit')\n",
    "# axs 2\n",
    "axs[2].hist(stdzd_samples, bins=80, color='blue', edgecolor='black', alpha=1)\n",
    "axs[2].hist(stdized_rewards, bins=80, color='orange', edgecolor='black', alpha=0.8)\n",
    "axs[2].set_title('Histogram standardized rewards and samples')\n",
    "axs[2].legend(['log-normal samples', 'negative baseline rewards'])\n",
    "# axs 3\n",
    "stats.probplot(stdized_rewards, dist=stats.norm, plot=axs[3])\n",
    "axs[3].set_title('Q-Q plot stdzd rewards vs standard normal')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the graphs above, the three-parameter log-normal fit seem to reasonably model the distribution of negative reward. The Q-Q plots indicates a lighter right tail and a more left skewed distribution of negative rewards than the theoretial log-normal fit, meaning that the fit models higher values than observed. On the reward scale, this means lower rewards than what was actually observed. The lighter right tail of negativ reward distribution is also apparent from the boxlplot. This assessment does not replace a formal gof test. However, the calculated confidence interval based on the log-normal fit very closely resembles the CI's from the other two methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
