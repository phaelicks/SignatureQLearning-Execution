{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bde00e1",
   "metadata": {},
   "source": [
    "# Execution Results Analysis\n",
    "\n",
    "This notebook analyses the results obtained from the Signature Q-Learning execution experiment, as carried out in the companion notebook `execution_training_testing.ipynb`. It covers:\n",
    "\n",
    "1. **Baseline policy analysis** — summary statistics, reward distribution, and confidence intervals for the baseline (sell-inventory) policy.\n",
    "2. **Training analysis** — per-run and averaged training trajectories (reward, loss, cash, terminal inventory), observation-action histories, and convergence diagnostics via first-observation values.\n",
    "3. **Testing analysis** — summary statistics, inventory/action trajectories, and reward comparisons across test runs.\n",
    "\n",
    "The results loaded here correspond to `date_id = '20250127_A'` and are stored in the `../results` directory. A detailed discussion of these results is provided in **Section 6.3** of the master thesis. To analyse results from a different experiment, change the `date_id` variable in the cell below; note, however, that the inline comments in this notebook refer to the `20250127_A` run.\n",
    "\n",
    "**Note**: To save any plot, set `save=True` in the respective plotting function call. Plots are saved to the `../figures` directory with the `date_id` included in the file name.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872535e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src') # to import from src directory\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import scipy.stats as stats \n",
    "\n",
    "import plotting_utils\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90898d48",
   "metadata": {},
   "source": [
    "# Load results\n",
    "\n",
    "Load baseline, training, and test results from saved pickle files with selected `date_id`. Set the `date_id` in the cell below to select which results to load. All results are expected to reside in the `../results` directory and are loaded via `utils.load_results`.\n",
    "\n",
    "Results from directory `../results` used in the master thesis and in the analysis below:\n",
    "\n",
    "- baseline policy: `execution_baseline_results_20250127_A.pkl`\n",
    "- training: `execution_training_results_20250127_A.pkl`\n",
    "- testing: `execution_test_results_20250127_A.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7433db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set date_id of results to be loaded\n",
    "date_id = '20250127_A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bed399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load baseline policy results\n",
    "baseline_results_dict = utils.load_results(date_id, results_type='baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f22062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training results\n",
    "training_data = utils.load_results(date_id, results_type='training')\n",
    "\n",
    "# Unpack with helper function\n",
    "(training_results_dict, final_Q_functions, sigq_params, \n",
    "    training_params, env_params, training_seeds, n_runs) = utils.unpack_training_results(training_data)\n",
    "\n",
    "# Display parameters\n",
    "print(f'\\nLoaded {n_runs} training runs with parameters:')\n",
    "from pprint import pprint\n",
    "pprint({\n",
    "    k: v for k, v in training_data.items() \n",
    "    if k not in ('training_results', 'final_Q_functions')\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f73f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test results\n",
    "testing_data = utils.load_results(date_id, results_type='testing')\n",
    "\n",
    "# Display seeds and checkpoint\n",
    "print(f'Loaded {n_runs} test runs for training checkpoint: {testing_data[\"checkpoint\"]},' \\\n",
    "      f' \\n and with environment seeds: {testing_data[\"test_seeds\"]}')\n",
    "\n",
    "# We only need test runs results\n",
    "test_results_dict, _, _ = testing_data.values()\n",
    "del testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9aa8e9",
   "metadata": {},
   "source": [
    "---\n",
    "# Baseline policy analysis\n",
    "\n",
    "The baseline policy sells the starting inventory at a fixed rate until the absolute inventory falls below a threshold (5 % of the maximum inventory) and then stops trading. It serves as a benchmark against which the Signature Q-Learning agent is compared. In this section we compute summary statistics, plot reward and inventory trajectories, inspect the reward distribution, and construct confidence intervals for the mean baseline reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac5a9c",
   "metadata": {},
   "source": [
    "## Baseline statistics\n",
    "\n",
    "Compute and display summary statistics of the baseline policy: mean, standard deviation, and median of the episode rewards, as well as mean, standard deviation, minimum, and maximum of the terminal inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a607c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_stats = []\n",
    "baseline_stats.append(np.mean(baseline_results_dict['rewards'])) # mean reward\n",
    "baseline_stats.append(np.std(baseline_results_dict['rewards'])) # std reward\n",
    "baseline_stats.append(np.median(baseline_results_dict['rewards'])) # median reward\n",
    "baseline_stats.append(np.mean(baseline_results_dict['terminal_inventories'])) # mean terminal inventory\n",
    "baseline_stats.append(np.std(baseline_results_dict['terminal_inventories'])) # std terminal inventory\n",
    "baseline_stats.append(int(np.min(baseline_results_dict['terminal_inventories']))) # min terminal inventory\n",
    "baseline_stats.append(int(np.max(baseline_results_dict['terminal_inventories']))) # max terminal inventory\n",
    "    \n",
    "columns = [\n",
    "    'Mean\\nreward', 'Std\\nreward', 'Median\\nreward', 'Mean terminal\\ninventory', \n",
    "    'Std terminal\\ninventory', 'Min terminal\\ninventory', 'Max terminal\\ninventory' \n",
    "]\n",
    "print(tabulate([baseline_stats], headers=columns, tablefmt='fancy_grid', floatfmt='.5f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c0ad0b",
   "metadata": {},
   "source": [
    "## Baseline plots\n",
    "\n",
    "Overview plot of the baseline run (rewards, terminal inventories, actions, and inventories for a selected episode) and separate trajectory plots for selected episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de694973",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_id = -1\n",
    "plotting_utils.plot_baseline_results(baseline_results_dict, episode_id, ma_window=20, \n",
    "                                     figsize=(6, 4), date_id=date_id, save=False)\n",
    "\n",
    "plotting_utils.plot_baseline_trajectories(baseline_results_dict, episode_ids=[-2, -3], \n",
    "                                          date_id=date_id, save=True, show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027d2d5",
   "metadata": {},
   "source": [
    "## Confidence intervals of baseline rewards\n",
    "\n",
    "We inspect the distribution of rewards from the baseline run through plotting a histogram and boxplot and find that it is moderately right-skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rewards = np.array(baseline_results_dict['rewards'])\n",
    "plotting_utils.plot_baseline_reward_distribution(baseline_rewards, figsize=(10, 3.5), \n",
    "                                                 date_id=date_id, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca927c1",
   "metadata": {},
   "source": [
    "We compute confidence intervals for the mean reward using three different methods: \n",
    "1. Normal-based CI (Large Sample Theory),   \n",
    "2. Bootstrap CI,\n",
    "3. Log-Transformed CI.\n",
    "\n",
    "The log-transformed CI is computed by fitting a lognormal distribution to the negative rewards (i.e. positive values) and following the approach for confidence intervals for three parameter log-normal distrobutions in the paper by Olsson (2017). The obtained CI are then transformed back to the original scale. The bootstrap CI is obtained by resampling the rewards with replacement and computing the mean of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68219062",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rewards = np.array(baseline_results_dict['rewards']) # Ensure the data is a NumPy array\n",
    "\n",
    "alpha = 0.05 # significance level 1 - alpha\n",
    "n = len(baseline_rewards)\n",
    "t_critical = stats.t.ppf(1 - alpha / 2, df=n-1)  # 95% confidence level\n",
    "\n",
    "# compute skewness\n",
    "sample_skewness = stats.skew(baseline_rewards)\n",
    "print('Reward distribution skewness:', round(sample_skewness, 5))\n",
    "\n",
    "# 1. Normal-based CI (Large Sample Theory)\n",
    "sample_mean = np.mean(baseline_rewards)\n",
    "sample_std = np.std(baseline_rewards, ddof=1)\n",
    "print('Standard error:', round(sample_std / np.sqrt(n), 5))\n",
    "\n",
    "normal_CI = (sample_mean - t_critical * sample_std / np.sqrt(n), \n",
    "             sample_mean + t_critical * sample_std / np.sqrt(n))\n",
    "\n",
    "# 2. Bootstrap CI\n",
    "np.random.seed(1) \n",
    "bootstrap_samples = np.random.choice(baseline_rewards, size=(1000, n), replace=True)\n",
    "bootstrap_means = np.mean(bootstrap_samples, axis=1)\n",
    "bootstrap_CI = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "# 3. Log-Transformed CI\n",
    "shape, loc, scale = stats.lognorm.fit(-baseline_rewards, loc=0.0)\n",
    "stdized_rewards = (np.log(-baseline_rewards - loc) - np.log(scale)) / shape\n",
    "fitted_mean = np.log(scale) # mean of (- rewards - loc), lognormal with loc=0\n",
    "fitted_std_estimate = np.sqrt(shape ** 2 / n + shape ** 4 / (2*(n - 1))) # based on Olsson paper\n",
    "\n",
    "lognorm_CI = (np.exp(fitted_mean + shape ** 2 / 2 - t_critical * fitted_std_estimate), \n",
    "              np.exp(fitted_mean + shape ** 2 / 2 + t_critical * fitted_std_estimate))\n",
    "lognorm_CI = (-lognorm_CI[1] - loc, -lognorm_CI[0] - loc)\n",
    "\n",
    "# print results\n",
    "CI_results = [normal_CI, bootstrap_CI, lognorm_CI]\n",
    "print(tabulate(CI_results, headers=[f'{int(100*(1-alpha))}% Confidence intervals', 'lower bound', 'upper bound'], \n",
    "               showindex=['normal CI', 'bootstrap CI', 'lognormal CI'], tablefmt='fancy_grid', floatfmt='.6f'))\n",
    "\n",
    "lognorm_params = np.array([shape, loc, scale])\n",
    "print(tabulate([lognorm_params], headers=['shape', 'loc', 'scale'], showindex=['lognormal fit'],\n",
    "               tablefmt='fancy_grid', floatfmt='.5f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe94771",
   "metadata": {},
   "source": [
    "## Baseline log-normal fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71276a",
   "metadata": {},
   "source": [
    "We inspect the log-normal fit by visual means. No statistical goodness-of-fit test is performed, however, one could transform the data with the fitted paramters to the normal scale and test for normality with e.g. Anderson-Darling, Shapiro-Wilk, Jarque-Bera test.\n",
    "Note, that performing these tests with parameters fitted from the data changes the test value's dsitrbution under the null and p-values might be meaningless. \n",
    "\n",
    "Alternatively, parametric bootstrapping could be performed with a chosen test, to obtain the empirical distribution of the test value under the null distribution with the fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ba57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual inspection of log-normal fit\n",
    "plotting_utils.plot_baseline_lognormal_fit(baseline_rewards, shape, loc, scale, \n",
    "                                           figsize=(10, 9), date_id=date_id, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c289f81",
   "metadata": {},
   "source": [
    "Judging from the graphs above, the three-parameter log-normal fit seems to reasonably model the distribution of negative rewards. The Q-Q plot indicates a lighter right tail and a more left-skewed distribution of negative rewards than the theoretical log-normal fit, meaning that the fit models higher values than observed. On the reward scale, this corresponds to lower rewards than what was actually observed. The lighter right tail of the negative reward distribution is also apparent from the boxplot. This assessment does not replace a formal goodness-of-fit test. However, the calculated confidence interval based on the log-normal fit very closely resembles the CIs from the other two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a07d8",
   "metadata": {},
   "source": [
    "---\n",
    "# Training analysis\n",
    "\n",
    "This section analyses the training phase of the Signature Q-Learning algorithm. We inspect individual training runs, the averaged training metrics across all runs, the observation-action trajectories at selected episodes, and the convergence of the first-observation value towards the mean episode reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801084f9",
   "metadata": {},
   "source": [
    "## Single training run\n",
    "\n",
    "Plot the training trajectory (reward, loss, cash, terminal inventory) of a selected run. Additionally, a closer look at the terminal inventory in the final episodes is shown to assess whether the agent learned to liquidate its position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e6d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select training run number for plotting\n",
    "run_id = 3\n",
    "plotting_utils.plot_training_run_results(training_results_dict, run_id, \n",
    "                                         figsize=None, date_id=date_id,save=True)\n",
    "\n",
    "# a closer look at terminal inventory trajectory of the selected run\n",
    "start, end = -1500, -1\n",
    "plt.plot(training_results_dict[run_id]['terminal_inventory'][start:end])\n",
    "plt.plot([0 for _ in range(end - start)], color='black')\n",
    "plt.plot([100 for _ in range(end - start)], color='black')\n",
    "plt.plot([-100 for _ in range(end - start)], color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b851b",
   "metadata": {},
   "source": [
    "## Averaged training results\n",
    "\n",
    "Plot the mean training metrics (reward, loss, cash, terminal inventory) averaged over all runs, together with ±1 standard deviation bands. When `save=True`, each subplot is also saved as an individual file in `../figures` with the `date_id` in the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_utils.plot_mean_training_results(training_results_dict, figsize=None, \n",
    "                                          date_id=date_id, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b908eb",
   "metadata": {},
   "source": [
    "## Observation-action histories\n",
    "\n",
    "To gain a more qualitative insight into the policies based on the learned Q-functions, we plot the observation and action trajectories of specific training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb0de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ids = 'all'\n",
    "episode_ids = [0, 999, 1999, 2999] \n",
    "plotting_utils.plot_inventory_action_histories(training_results_dict, run_ids, episode_ids, \n",
    "                                               figsize=(8,8), date_id=date_id, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b7aacb",
   "metadata": {},
   "source": [
    "## First observation value convergence\n",
    "\n",
    "A key convergence diagnostic: compare the first-observation value $\\hat{Q}(o_0, \\cdot)$ provided by the learned Q-function at the start of each training episode with the mean episode reward. If both quantities converge towards the same value, this indicates that the Q-function approximation at the initial observation is consistent with the observed returns. Note that this does not guarantee convergence of the approximate Q-function $\\hat{Q}$ to the trwu Q-functions at all time steps, since only the value at the fixed starting observation is considered.\n",
    "\n",
    "The first plot shows the trajectory of the first-observation value across training episodes (mean ± std over all runs), overlaid with the mean baseline reward. The second plot compares the mean episode reward and the mean first-observation value over a window of the final training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac33104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_baseline_reward = baseline_stats[0]\n",
    "plotting_utils.plot_first_observation_values(training_results_dict, run_ids='all', mean=True, \n",
    "                                             std=True, figsize=(8,3.2), line=mean_baseline_reward,\n",
    "                                             date_id=date_id, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f271ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_baseline_reward = baseline_stats[0]\n",
    "plotting_utils.plot_reward_vs_first_obs_value(training_results_dict, episode_window=(-1000,-1), \n",
    "                                              figsize=(8,3), line=mean_baseline_reward,\n",
    "                                              date_id=date_id, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f6e06",
   "metadata": {},
   "source": [
    "---\n",
    "# Testing analysis\n",
    "\n",
    "This section evaluates the learned policies by running the final Q-functions on unseen environment episodes. Summary statistics, inventory and action trajectories, and reward distributions are reported and compared to the baseline policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd050949",
   "metadata": {},
   "source": [
    "## Test statistics\n",
    "\n",
    "The following statistics are reported for each test run:\n",
    "- Mean, standard deviation, and median of episode rewards\n",
    "- Mean and standard deviation of terminal inventory\n",
    "- Minimum and maximum terminal inventory\n",
    "- Percentage of episodes with terminal inventory in $[-\\rho/2,\\, \\rho/2]$ and $[-\\rho,\\, \\rho]$, with $\\rho$ being 10 % of maximum inventory\n",
    "- First-observation value of the Q-function at the last test episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b7d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats = []\n",
    "rho = 50\n",
    "for test_run in test_results_dict.values():\n",
    "    run_stats = []\n",
    "    run_stats.append(np.mean(test_run['rewards'])) # mean reward\n",
    "    run_stats.append(np.std(test_run['rewards'])) # std reward\n",
    "    run_stats.append(np.median(test_run['rewards'])) # median reward\n",
    "    run_stats.append(np.mean(test_run['terminal_inventories'])) # mean terminal inventory\n",
    "    run_stats.append(np.std(test_run['terminal_inventories'])) # std terminal inventory\n",
    "    run_stats.append(int(np.min(test_run['terminal_inventories']))) # min terminal inventory\n",
    "    run_stats.append(int(np.max(test_run['terminal_inventories']))) # max terminal inventory\n",
    "    run_stats.append(sum(abs(np.array(test_run['terminal_inventories'])) <= rho//2) / \n",
    "                     len(test_run['terminal_inventories']) ) # pct in [-rho/2, rho/2]\n",
    "    run_stats.append(sum(abs(np.array(test_run['terminal_inventories'])) <= rho) / \n",
    "                     len(test_run['terminal_inventories']) ) # pct in [-rho, rho]\n",
    "    run_stats.append(test_run['first_obs_values'][-1]) # first observation value\n",
    "    test_stats.append(run_stats)\n",
    "\n",
    "columns = ['\\nRun', 'Mean\\nreward', 'Std\\nreward', 'Median\\nreward','Mean terminal\\ninventory', \n",
    "           'Std terminal\\ninventory', 'Min terminal\\ninventory', 'Max terminal\\ninventory', \n",
    "           'Pct in\\n[-rho/2,rho/2]', 'Pct in\\n[-rho,rho]', 'First obs\\nvalue']\n",
    "print(tabulate(test_stats, headers=columns, showindex=test_results_dict.keys(),tablefmt='simple', floatfmt='.5f'))\n",
    "\n",
    "print(f'\\nmean of mean rewards:    {round(np.mean([run_stats[0] for run_stats in test_stats]), 5)}' \\\n",
    "      f'\\nmean of median rewards:  {round(np.mean([run_stats[2] for run_stats in test_stats]), 5)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc656ab6",
   "metadata": {},
   "source": [
    "## Test plots\n",
    "\n",
    "### Single test run and episode\n",
    "\n",
    "Plot rewards, terminal inventories, actions, and inventories for a selected test run and episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 0\n",
    "episode_id = -1\n",
    "plotting_utils.plot_test_run_results(test_results_dict, run_id, episode_id, ma_window=10, \n",
    "                                     figsize=(6, 4), date_id=date_id, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c672d65",
   "metadata": {},
   "source": [
    "### Inventory and action trajectories across test runs\n",
    "\n",
    "Plot the step-by-step inventory and action trajectories for all test episodes of the selected runs. These plots give a visual impression of how consistently the learned policies liquidate inventory across different environment seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d82df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectories of inventories for all test runs, takes a while to plot\n",
    "runs = list(range(10)) # which runs to plot\n",
    "plotting_utils.plot_test_inventory_trajectories(test_results_dict, runs=runs, figsize=(9, 14), \n",
    "                                                date_id=date_id, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectories of actions for all test runs, takes a while to plot\n",
    "runs = list(range(10)) # which runs to plot\n",
    "plotting_utils.plot_test_action_trajectories(test_results_dict, figsize=(8, 14), runs=runs, \n",
    "                                             date_id=date_id, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f6638",
   "metadata": {},
   "source": [
    "### Reward box-plots\n",
    "\n",
    "Box-plots comparing the reward distributions of each test run with the baseline reward distribution. This gives a direct visual comparison of the learned policy's performance against the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_utils.plot_test_rewards_boxplot(test_results_dict, baseline_results_dict, \n",
    "                                         date_id=date_id, save=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
