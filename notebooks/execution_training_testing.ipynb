{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50748498",
   "metadata": {},
   "source": [
    "# Execution Training & Testing\n",
    "\n",
    "This notebook runs the Signature Q-Learning experiment for the optimal execution problem. It covers the full pipeline from environment setup to saving results:\n",
    "\n",
    "1. **Environment setup** — register the custom execution environment as a Gym environment and define its parameters.\n",
    "2. **Baseline policy** — run a simple sell-inventory baseline and save its results.\n",
    "3. **Training** — learn approximate Q-functions with Signature Q-Learning over multiple seeded runs.\n",
    "4. **Testing** — evaluate the learned Q-functions on unseen episodes and save the test results.\n",
    "\n",
    "All results (baseline, training, testing) are saved as pickle files in the `../results` directory. Each file name includes a `date_id` (e.g. `20250127_A`) that is generated at the start of the notebook and serves as a unique identifier. The companion notebook `execution_results_analysis.ipynb` loads these files for analysis.\n",
    "\n",
    "Under the hood, each environment episode runs a full multi-agent market simulation powered by the ABIDES submodule (`abides-jpmc-public`). The environment wraps ABIDES as an OpenAI Gym environment via the custom class `SubGymMarketsCustomExecutionEnv`.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, '../src') # to import from src directory\n",
    "\n",
    "import gym\n",
    "\n",
    "from abides_gym_custom_execution_environment import SubGymMarketsCustomExecutionEnv\n",
    "from sigqlearning_qfunctions import SigQFunction\n",
    "from sigqlearning_test_execution import test\n",
    "from sigqlearning_train_execution import train\n",
    "from sigqlearning_baseline_execution import run_baseline\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1194646",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "\n",
    "## Register Gym environment\n",
    "\n",
    "Register the custom execution environment for Gym use and define a helper function to generate an environment instance with a given seed and parameters. Each call to `generate_env` creates a fresh environment backed by an ABIDES market simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c80cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='custom-execution-v0',\n",
    "    entry_point=SubGymMarketsCustomExecutionEnv,\n",
    ")\n",
    "\n",
    "def generate_env(seed=None, **env_params):\n",
    "    \"\"\"\n",
    "    generates specific environment with the parameters defined and set the seed\n",
    "    \"\"\"\n",
    "    env = gym.make(id = 'custom-execution-v0', **env_params)\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50fdba",
   "metadata": {},
   "source": [
    "## Environment parameters\n",
    "\n",
    "Set the execution environment parameters used for training and testing. These parameters correspond to the environment configuration detailed in **Section 6.3** of the master thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91fc45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_params = dict(\n",
    "    background_config = 'rmsc04',\n",
    "    mkt_close = '10:05:00',\n",
    "    timestep_duration = '10s',\n",
    "    first_interval = '00:05:00',\n",
    "    observation_interval = '00:00:00',            \n",
    "    order_fixed_size = 50,\n",
    "    max_inventory = 1000,\n",
    "    starting_inventory = 700,\n",
    "    terminal_inventory_reward = -0.7, # reward or penalty\n",
    "    terminal_inventory_mode = 'quadratic', # quadratic, linear, flat\n",
    "    running_inventory_reward_dampener = 0., # 0.6, 1.0\n",
    "    damp_mode = None, # asymmetric\n",
    "    debug_mode = False,\n",
    "    reward_multiplier = 'quadratic_positive', # running reward mode\n",
    "    reward_multiplier_float = None, #1.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa4a81",
   "metadata": {},
   "source": [
    "## Date identifier\n",
    "\n",
    "Generate a `date_id` (e.g. `'20250127_A'`) that uniquely identifies all results saved during this notebook run. The identifier is based on the current date and an incrementing letter to avoid overwriting existing files in `../results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7340f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns current date plus letter as string\n",
    "date_id = utils.get_date_id() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ac9132",
   "metadata": {},
   "source": [
    "---\n",
    "# Baseline policy\n",
    "\n",
    "The baseline policy sells the starting inventory at a fixed rate as long as the absolute inventory is above 5 % of the maximum inventory and then stops trading until the end of the episode. This is the policy the RL agent is supposed to learn, and the resulting baseline reward serves as a benchmark for comparison. Results are saved to `../results` via `utils.save_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run baseline\n",
    "env = generate_env(1000, **env_params) # seed different from any test run seed\n",
    "baseline_results_dict = run_baseline(env, episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save baseline results\n",
    "date_id = utils.save_results(baseline_results_dict, date_id, results_type='baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb05cf",
   "metadata": {},
   "source": [
    "---\n",
    "# Training\n",
    "\n",
    "Train approximate Q-functions using Signature Q-Learning. The training is performed over multiple independent runs with different seeds to assess variability. Each run produces a training results dictionary and a final Q-function state dict.\n",
    "\n",
    "## Training parameters\n",
    "\n",
    "Define the signature Q-function parameters (truncation depth, basepoint, initial bias) and the training hyper-parameters (number of episodes, discount factor, learning rate and its decay, exploration strategy with epsilon-greedy decay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01110641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature parameters\n",
    "sigq_params = dict(\n",
    "    sig_depth = 7,\n",
    "    basepoint = [0., 0.], \n",
    "    initial_bias = 0.01,\n",
    ")\n",
    "\n",
    "# training parameter\n",
    "training_params = dict(\n",
    "    episodes = 2,\n",
    "    discount = 1.0,\n",
    "    learning_rate = 5*1e-5,\n",
    "    learning_rate_decay = dict(mode='exponential', factor=0.999),\n",
    "    exploration = 'greedy',\n",
    "    epsilon = 1,\n",
    "    epsilon_decay = dict(mode='exponential', factor=0.997),\n",
    "    decay_mode = 'episodes',\n",
    "    debug_mode = None,\n",
    "    progress_display = 'tqdm', # set to 'livelossplot' for live charts during each training run\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b117554a",
   "metadata": {},
   "source": [
    "## Learn Q-function estimates\n",
    "\n",
    "An approximate Q-function is learned with Signature Q-Learning. Multiple runs are executed with distinct prime-number seeds to ensure reproducibility and to capture variance across initialisations.\n",
    "\n",
    "**Note**: This cell takes a long time to run, as each episode involves a full ABIDES market simulation. Consider copying this cell into a standalone Python script and running it in the background (e.g. with [screen](https://linuxize.com/post/how-to-use-linux-screen/) or [tmux](https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faac56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training runs\n",
    "n_runs = 2\n",
    "\n",
    "# dict to store results\n",
    "training_results_dict = {run : [] for run in range(n_runs)}\n",
    "training_seeds = {\n",
    "    run : seed for run, seed \n",
    "    in zip(training_results_dict, utils.generate_prime_seeds(n_runs, random=False))\n",
    "}\n",
    "\n",
    "final_Q_functions = {}\n",
    "\n",
    "# training runs\n",
    "runs_pbar = tqdm(training_results_dict.keys(), desc='Training run')\n",
    "for run in runs_pbar:\n",
    "    env = generate_env(training_seeds[run], **env_params)\n",
    "    sigqfunction = SigQFunction(env, **sigq_params)\n",
    "    training_results_dict[run] = train(env, sigqfunction, **training_params)\n",
    "    final_Q_functions[run] = sigqfunction.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b13b8",
   "metadata": {},
   "source": [
    "## Save training results\n",
    "\n",
    "Save the training results (per-run results, final Q-function state dicts, and all parameters) to `../results` as a single pickle file. The file name follows the pattern `execution_training_results_<date_id>.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb42851",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_collected=dict(\n",
    "    training_results=training_results_dict, \n",
    "    final_Q_functions=final_Q_functions,\n",
    "    sig_params=sigq_params, \n",
    "    training_params=training_params, \n",
    "    env_params=env_params,\n",
    "    training_seeds=training_seeds\n",
    ")\n",
    "\n",
    "date_id = utils.save_results(results_collected, date_id, results_type='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f5e34",
   "metadata": {},
   "source": [
    "---\n",
    "# Testing\n",
    "\n",
    "Evaluate the learned Q-functions on unseen environment episodes. To test Q-functions from a previous training run instead of the one just completed, set `load_training_results` to `True` and specify the corresponding `date_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f406986",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_training_results = False\n",
    "\n",
    "if load_training_results:\n",
    "    date_id = '20250127_A'\n",
    "    training_data = utils.load_results('training', date_id)\n",
    "    \n",
    "    # Unpack with helper function\n",
    "    (training_results_dict, final_Q_functions, sigq_params, \n",
    "     training_params, env_params, training_seeds, n_runs) = utils.unpack_training_results(training_data)\n",
    "    \n",
    "    # Display parameters\n",
    "    print(f'Loaded {n_runs} training runs with parameters:')\n",
    "    from pprint import pprint\n",
    "    pprint({\n",
    "        k: v for k, v in training_data.items() \n",
    "        if k not in ('training_results', 'final_Q_functions')\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4daa4e4",
   "metadata": {},
   "source": [
    "## Test Q-function estimates\n",
    "\n",
    "Run the learned Q-functions greedily (no exploration) on fresh environment episodes. By default the final Q-function from each training run is used (`checkpoint = -1`). To test an intermediate checkpoint saved during training, set `checkpoint` to the desired index (checkpoints are saved every 10 episodes during training, so `0 <= checkpoint < n_training_episodes / 10`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = -1 # final\n",
    "\n",
    "n_test_episodes = 2000\n",
    "test_results_dict = {}\n",
    "test_seeds = utils.generate_prime_seeds(100, random=True)\n",
    "\n",
    "runs_pbar = tqdm(training_results_dict.keys(), desc='Test run')\n",
    "for run in runs_pbar:\n",
    "    env = generate_env(test_seeds[run], **env_params)\n",
    "    sigqfunction = SigQFunction(env, **sigq_params)\n",
    "    sigq_state_dict = final_Q_functions[run] if checkpoint == -1 \\\n",
    "        else training_results_dict[run]['intermediate'][checkpoint]\n",
    "    sigqfunction.load_state_dict(sigq_state_dict)\n",
    "    test_results_dict[run] = test(env, sigqfunction, n_test_episodes, debug_mode='info')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef15b69d",
   "metadata": {},
   "source": [
    "## Save test results\n",
    "\n",
    "Save the test results (per-run results, test seeds, and the checkpoint used) to `../results` as a pickle file. The file name follows the pattern `execution_test_results_<date_id>.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb110fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = {\n",
    "    'test_results': test_results_dict,\n",
    "    'test_seeds': test_seeds,\n",
    "    'checkpoint': checkpoint,\n",
    "}\n",
    "\n",
    "date_id = utils.save_results(testing_data, date_id='20250127_A', results_type='testing')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
